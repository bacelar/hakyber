require "params.jinc"
require "consts.jinc"
require "shuffle.jinc"
require "fips202_avx2.jinc"
require "fips202_4x.jinc"

param int GENMATRIX_NBLOCKS = 3; // ((12*KYBER_N/8*4096/KYBER_Q + SHAKE128_RATE)/SHAKE128_RATE);
param int REJ_BUFLEN = 512; // GENMATRIX_NBLOCKS * SHAKE128_RATE;

u8[2048] ru_idx = {-1, -1, -1, -1, -1, -1, -1, -1,
                0, -1, -1, -1, -1, -1, -1, -1,
                2, -1, -1, -1, -1, -1, -1, -1,
                0,  2, -1, -1, -1, -1, -1, -1,
                4, -1, -1, -1, -1, -1, -1, -1,
                0,  4, -1, -1, -1, -1, -1, -1,
                2,  4, -1, -1, -1, -1, -1, -1,
                0,  2,  4, -1, -1, -1, -1, -1,
                6, -1, -1, -1, -1, -1, -1, -1,
                0,  6, -1, -1, -1, -1, -1, -1,
                2,  6, -1, -1, -1, -1, -1, -1,
                0,  2,  6, -1, -1, -1, -1, -1,
                4,  6, -1, -1, -1, -1, -1, -1,
                0,  4,  6, -1, -1, -1, -1, -1,
                2,  4,  6, -1, -1, -1, -1, -1,
                0,  2,  4,  6, -1, -1, -1, -1,
                8, -1, -1, -1, -1, -1, -1, -1,
                0,  8, -1, -1, -1, -1, -1, -1,
                2,  8, -1, -1, -1, -1, -1, -1,
                0,  2,  8, -1, -1, -1, -1, -1,
                4,  8, -1, -1, -1, -1, -1, -1,
                0,  4,  8, -1, -1, -1, -1, -1,
                2,  4,  8, -1, -1, -1, -1, -1,
                0,  2,  4,  8, -1, -1, -1, -1,
                6,  8, -1, -1, -1, -1, -1, -1,
                0,  6,  8, -1, -1, -1, -1, -1,
                2,  6,  8, -1, -1, -1, -1, -1,
                0,  2,  6,  8, -1, -1, -1, -1,
                4,  6,  8, -1, -1, -1, -1, -1,
                0,  4,  6,  8, -1, -1, -1, -1,
                2,  4,  6,  8, -1, -1, -1, -1,
                0,  2,  4,  6,  8, -1, -1, -1,
                10, -1, -1, -1, -1, -1, -1, -1,
                0, 10, -1, -1, -1, -1, -1, -1,
                2, 10, -1, -1, -1, -1, -1, -1,
                0,  2, 10, -1, -1, -1, -1, -1,
                4, 10, -1, -1, -1, -1, -1, -1,
                0,  4, 10, -1, -1, -1, -1, -1,
                2,  4, 10, -1, -1, -1, -1, -1,
                0,  2,  4, 10, -1, -1, -1, -1,
                6, 10, -1, -1, -1, -1, -1, -1,
                0,  6, 10, -1, -1, -1, -1, -1,
                2,  6, 10, -1, -1, -1, -1, -1,
                0,  2,  6, 10, -1, -1, -1, -1,
                4,  6, 10, -1, -1, -1, -1, -1,
                0,  4,  6, 10, -1, -1, -1, -1,
                2,  4,  6, 10, -1, -1, -1, -1,
                0,  2,  4,  6, 10, -1, -1, -1,
                8, 10, -1, -1, -1, -1, -1, -1,
                0,  8, 10, -1, -1, -1, -1, -1,
                2,  8, 10, -1, -1, -1, -1, -1,
                0,  2,  8, 10, -1, -1, -1, -1,
                4,  8, 10, -1, -1, -1, -1, -1,
                0,  4,  8, 10, -1, -1, -1, -1,
                2,  4,  8, 10, -1, -1, -1, -1,
                0,  2,  4,  8, 10, -1, -1, -1,
                6,  8, 10, -1, -1, -1, -1, -1,
                0,  6,  8, 10, -1, -1, -1, -1,
                2,  6,  8, 10, -1, -1, -1, -1,
                0,  2,  6,  8, 10, -1, -1, -1,
                4,  6,  8, 10, -1, -1, -1, -1,
                0,  4,  6,  8, 10, -1, -1, -1,
                2,  4,  6,  8, 10, -1, -1, -1,
                0,  2,  4,  6,  8, 10, -1, -1,
                12, -1, -1, -1, -1, -1, -1, -1,
                0, 12, -1, -1, -1, -1, -1, -1,
                2, 12, -1, -1, -1, -1, -1, -1,
                0,  2, 12, -1, -1, -1, -1, -1,
                4, 12, -1, -1, -1, -1, -1, -1,
                0,  4, 12, -1, -1, -1, -1, -1,
                2,  4, 12, -1, -1, -1, -1, -1,
                0,  2,  4, 12, -1, -1, -1, -1,
                6, 12, -1, -1, -1, -1, -1, -1,
                0,  6, 12, -1, -1, -1, -1, -1,
                2,  6, 12, -1, -1, -1, -1, -1,
                0,  2,  6, 12, -1, -1, -1, -1,
                4,  6, 12, -1, -1, -1, -1, -1,
                0,  4,  6, 12, -1, -1, -1, -1,
                2,  4,  6, 12, -1, -1, -1, -1,
                0,  2,  4,  6, 12, -1, -1, -1,
                8, 12, -1, -1, -1, -1, -1, -1,
                0,  8, 12, -1, -1, -1, -1, -1,
                2,  8, 12, -1, -1, -1, -1, -1,
                0,  2,  8, 12, -1, -1, -1, -1,
                4,  8, 12, -1, -1, -1, -1, -1,
                0,  4,  8, 12, -1, -1, -1, -1,
                2,  4,  8, 12, -1, -1, -1, -1,
                0,  2,  4,  8, 12, -1, -1, -1,
                6,  8, 12, -1, -1, -1, -1, -1,
                0,  6,  8, 12, -1, -1, -1, -1,
                2,  6,  8, 12, -1, -1, -1, -1,
                0,  2,  6,  8, 12, -1, -1, -1,
                4,  6,  8, 12, -1, -1, -1, -1,
                0,  4,  6,  8, 12, -1, -1, -1,
                2,  4,  6,  8, 12, -1, -1, -1,
                0,  2,  4,  6,  8, 12, -1, -1,
                10, 12, -1, -1, -1, -1, -1, -1,
                0, 10, 12, -1, -1, -1, -1, -1,
                2, 10, 12, -1, -1, -1, -1, -1,
                0,  2, 10, 12, -1, -1, -1, -1,
                4, 10, 12, -1, -1, -1, -1, -1,
                0,  4, 10, 12, -1, -1, -1, -1,
                2,  4, 10, 12, -1, -1, -1, -1,
                0,  2,  4, 10, 12, -1, -1, -1,
                6, 10, 12, -1, -1, -1, -1, -1,
                0,  6, 10, 12, -1, -1, -1, -1,
                2,  6, 10, 12, -1, -1, -1, -1,
                0,  2,  6, 10, 12, -1, -1, -1,
                4,  6, 10, 12, -1, -1, -1, -1,
                0,  4,  6, 10, 12, -1, -1, -1,
                2,  4,  6, 10, 12, -1, -1, -1,
                0,  2,  4,  6, 10, 12, -1, -1,
                8, 10, 12, -1, -1, -1, -1, -1,
                0,  8, 10, 12, -1, -1, -1, -1,
                2,  8, 10, 12, -1, -1, -1, -1,
                0,  2,  8, 10, 12, -1, -1, -1,
                4,  8, 10, 12, -1, -1, -1, -1,
                0,  4,  8, 10, 12, -1, -1, -1,
                2,  4,  8, 10, 12, -1, -1, -1,
                0,  2,  4,  8, 10, 12, -1, -1,
                6,  8, 10, 12, -1, -1, -1, -1,
                0,  6,  8, 10, 12, -1, -1, -1,
                2,  6,  8, 10, 12, -1, -1, -1,
                0,  2,  6,  8, 10, 12, -1, -1,
                4,  6,  8, 10, 12, -1, -1, -1,
                0,  4,  6,  8, 10, 12, -1, -1,
                2,  4,  6,  8, 10, 12, -1, -1,
                0,  2,  4,  6,  8, 10, 12, -1,
                14, -1, -1, -1, -1, -1, -1, -1,
                0, 14, -1, -1, -1, -1, -1, -1,
                2, 14, -1, -1, -1, -1, -1, -1,
                0,  2, 14, -1, -1, -1, -1, -1,
                4, 14, -1, -1, -1, -1, -1, -1,
                0,  4, 14, -1, -1, -1, -1, -1,
                2,  4, 14, -1, -1, -1, -1, -1,
                0,  2,  4, 14, -1, -1, -1, -1,
                6, 14, -1, -1, -1, -1, -1, -1,
                0,  6, 14, -1, -1, -1, -1, -1,
                2,  6, 14, -1, -1, -1, -1, -1,
                0,  2,  6, 14, -1, -1, -1, -1,
                4,  6, 14, -1, -1, -1, -1, -1,
                0,  4,  6, 14, -1, -1, -1, -1,
                2,  4,  6, 14, -1, -1, -1, -1,
                0,  2,  4,  6, 14, -1, -1, -1,
                8, 14, -1, -1, -1, -1, -1, -1,
                0,  8, 14, -1, -1, -1, -1, -1,
                2,  8, 14, -1, -1, -1, -1, -1,
                0,  2,  8, 14, -1, -1, -1, -1,
                4,  8, 14, -1, -1, -1, -1, -1,
                0,  4,  8, 14, -1, -1, -1, -1,
                2,  4,  8, 14, -1, -1, -1, -1,
                0,  2,  4,  8, 14, -1, -1, -1,
                6,  8, 14, -1, -1, -1, -1, -1,
                0,  6,  8, 14, -1, -1, -1, -1,
                2,  6,  8, 14, -1, -1, -1, -1,
                0,  2,  6,  8, 14, -1, -1, -1,
                4,  6,  8, 14, -1, -1, -1, -1,
                0,  4,  6,  8, 14, -1, -1, -1,
                2,  4,  6,  8, 14, -1, -1, -1,
                0,  2,  4,  6,  8, 14, -1, -1,
                10, 14, -1, -1, -1, -1, -1, -1,
                0, 10, 14, -1, -1, -1, -1, -1,
                2, 10, 14, -1, -1, -1, -1, -1,
                0,  2, 10, 14, -1, -1, -1, -1,
                4, 10, 14, -1, -1, -1, -1, -1,
                0,  4, 10, 14, -1, -1, -1, -1,
                2,  4, 10, 14, -1, -1, -1, -1,
                0,  2,  4, 10, 14, -1, -1, -1,
                6, 10, 14, -1, -1, -1, -1, -1,
                0,  6, 10, 14, -1, -1, -1, -1,
                2,  6, 10, 14, -1, -1, -1, -1,
                0,  2,  6, 10, 14, -1, -1, -1,
                4,  6, 10, 14, -1, -1, -1, -1,
                0,  4,  6, 10, 14, -1, -1, -1,
                2,  4,  6, 10, 14, -1, -1, -1,
                0,  2,  4,  6, 10, 14, -1, -1,
                8, 10, 14, -1, -1, -1, -1, -1,
                0,  8, 10, 14, -1, -1, -1, -1,
                2,  8, 10, 14, -1, -1, -1, -1,
                0,  2,  8, 10, 14, -1, -1, -1,
                4,  8, 10, 14, -1, -1, -1, -1,
                0,  4,  8, 10, 14, -1, -1, -1,
                2,  4,  8, 10, 14, -1, -1, -1,
                0,  2,  4,  8, 10, 14, -1, -1,
                6,  8, 10, 14, -1, -1, -1, -1,
                0,  6,  8, 10, 14, -1, -1, -1,
                2,  6,  8, 10, 14, -1, -1, -1,
                0,  2,  6,  8, 10, 14, -1, -1,
                4,  6,  8, 10, 14, -1, -1, -1,
                0,  4,  6,  8, 10, 14, -1, -1,
                2,  4,  6,  8, 10, 14, -1, -1,
                0,  2,  4,  6,  8, 10, 14, -1,
                12, 14, -1, -1, -1, -1, -1, -1,
                0, 12, 14, -1, -1, -1, -1, -1,
                2, 12, 14, -1, -1, -1, -1, -1,
                0,  2, 12, 14, -1, -1, -1, -1,
                4, 12, 14, -1, -1, -1, -1, -1,
                0,  4, 12, 14, -1, -1, -1, -1,
                2,  4, 12, 14, -1, -1, -1, -1,
                0,  2,  4, 12, 14, -1, -1, -1,
                6, 12, 14, -1, -1, -1, -1, -1,
                0,  6, 12, 14, -1, -1, -1, -1,
                2,  6, 12, 14, -1, -1, -1, -1,
                0,  2,  6, 12, 14, -1, -1, -1,
                4,  6, 12, 14, -1, -1, -1, -1,
                0,  4,  6, 12, 14, -1, -1, -1,
                2,  4,  6, 12, 14, -1, -1, -1,
                0,  2,  4,  6, 12, 14, -1, -1,
                8, 12, 14, -1, -1, -1, -1, -1,
                0,  8, 12, 14, -1, -1, -1, -1,
                2,  8, 12, 14, -1, -1, -1, -1,
                0,  2,  8, 12, 14, -1, -1, -1,
                4,  8, 12, 14, -1, -1, -1, -1,
                0,  4,  8, 12, 14, -1, -1, -1,
                2,  4,  8, 12, 14, -1, -1, -1,
                0,  2,  4,  8, 12, 14, -1, -1,
                6,  8, 12, 14, -1, -1, -1, -1,
                0,  6,  8, 12, 14, -1, -1, -1,
                2,  6,  8, 12, 14, -1, -1, -1,
                0,  2,  6,  8, 12, 14, -1, -1,
                4,  6,  8, 12, 14, -1, -1, -1,
                0,  4,  6,  8, 12, 14, -1, -1,
                2,  4,  6,  8, 12, 14, -1, -1,
                0,  2,  4,  6,  8, 12, 14, -1,
                10, 12, 14, -1, -1, -1, -1, -1,
                0, 10, 12, 14, -1, -1, -1, -1,
                2, 10, 12, 14, -1, -1, -1, -1,
                0,  2, 10, 12, 14, -1, -1, -1,
                4, 10, 12, 14, -1, -1, -1, -1,
                0,  4, 10, 12, 14, -1, -1, -1,
                2,  4, 10, 12, 14, -1, -1, -1,
                0,  2,  4, 10, 12, 14, -1, -1,
                6, 10, 12, 14, -1, -1, -1, -1,
                0,  6, 10, 12, 14, -1, -1, -1,
                2,  6, 10, 12, 14, -1, -1, -1,
                0,  2,  6, 10, 12, 14, -1, -1,
                4,  6, 10, 12, 14, -1, -1, -1,
                0,  4,  6, 10, 12, 14, -1, -1,
                2,  4,  6, 10, 12, 14, -1, -1,
                0,  2,  4,  6, 10, 12, 14, -1,
                8, 10, 12, 14, -1, -1, -1, -1,
                0,  8, 10, 12, 14, -1, -1, -1,
                2,  8, 10, 12, 14, -1, -1, -1,
                0,  2,  8, 10, 12, 14, -1, -1,
                4,  8, 10, 12, 14, -1, -1, -1,
                0,  4,  8, 10, 12, 14, -1, -1,
                2,  4,  8, 10, 12, 14, -1, -1,
                0,  2,  4,  8, 10, 12, 14, -1,
                6,  8, 10, 12, 14, -1, -1, -1,
                0,  6,  8, 10, 12, 14, -1, -1,
                2,  6,  8, 10, 12, 14, -1, -1,
                0,  2,  6,  8, 10, 12, 14, -1,
                4,  6,  8, 10, 12, 14, -1, -1,
                0,  4,  6,  8, 10, 12, 14, -1,
                2,  4,  6,  8, 10, 12, 14, -1,
                0,  2,  4,  6,  8, 10, 12, 14};

/*
inline
fn __shake128_squeezenblocks(stack u64[25] state, stack u8[REJ_BUFLEN] out)
      -> stack u64[25], stack u8[REJ_BUFLEN]
{
  inline int i;

  for i = 0 to GENMATRIX_NBLOCKS 
  {
      state, out[i*SHAKE128_RATE:SHAKE128_RATE] = _shake128_squeezeblock(state, out[i*SHAKE128_RATE:SHAKE128_RATE]);
  }
  return state, out;
}
*/

inline
fn __rej_uniform(reg ptr u16[KYBER_N] rp, reg u64 offset, reg ptr u8[SHAKE128_RATE] buf, inline int buflen) ->  reg u64, stack u16[KYBER_N]
{
  reg u16 val0 val1;
  reg u16 t;
  reg u64 pos ctr;
  reg u8 fl1 fl2;
  reg bool cf zf b;

  ctr = offset;
  pos = 0;

  _, cf, _, _, zf = #CMP_64(ctr, KYBER_N - 1);
  fl1 = #SETcc(cf || zf); //SETBE

  _, cf, _, _, zf = #CMP_64(pos, buflen - 3);
  fl2 = #SETcc(cf || zf);  //SETBE

  _, _, _, _, b = #TEST_8(fl1, fl2);

  while(!b)
  {
    val0 = (16u)buf[(int)pos];
    pos += 1;

    t   = (16u)buf[(int)pos];
    val1 = t;
    val1 >>= 4;

    t &= 0x0F;
    t <<= 8;
    val0 |= t;
    pos += 1;

    t   = (16u)buf[(int)pos];
    t <<= 4;
    val1 |= t;
    pos += 1;

    if(val0 < KYBER_Q)
    {
      rp[(int)ctr] = val0;
      ctr += 1;
    }

    if(ctr < KYBER_N)
    {
      if(val1 < KYBER_Q)
      {
        rp[(int)ctr] = val1;
        ctr += 1;
      }
    }

    _, cf, _, _, zf = #CMP_64(ctr, KYBER_N - 1);
    fl1 = #SETcc(cf || zf); //SETBE

    _, cf, _, _, zf = #CMP_64(pos, buflen - 3);
    fl2 = #SETcc(cf || zf);  //SETBE

    _, _, _, _, b = #TEST_8(fl1, fl2);
  }

  return ctr, rp;
}

u8 ru_ones_s = 1;
u16 ru_mask_s = 0x0FFF;
u8[32] ru_idx8_s = {0, 1, 1, 2, 3, 4, 4, 5,
                 6, 7, 7, 8, 9, 10, 10, 11,
                 4, 5, 5, 6, 7, 8, 8, 9,
                 10, 11, 11, 12, 13, 14, 14, 15};


inline
fn __write_u128_boundchk( reg ptr u16[KYBER_N] rp
			, reg u64 ctr
			, reg u128 data
			) -> reg ptr u16[KYBER_N] {
  reg u64 data_u64;
  if ( ctr <= KYBER_N-8 ) {
    rp.[u128 2*(int)ctr] = data;
  } else {
    data_u64 = #MOVV(data);
    if ( ctr <= KYBER_N-4 ) {
      rp.[u64 2*(int)ctr] = data_u64;
      data_u64 = #VPEXTR_64(data, 1);
      ctr += 4;
    }
    if ( ctr <= KYBER_N-2 ) {
      rp.[u32 2*(int)ctr] = (32u) data_u64;
      data_u64 >>= 32;
      ctr += 2;
    }
    if ( ctr <= KYBER_N-1 ) {
      rp.[u16 2*(int)ctr] = (16u) data_u64;
    }
  }
  return rp;
}

inline /* processes 2*24u8 (2*8*3u8) bytes
        */ 
fn __process2u192( reg ptr u16[KYBER_N] rp
		 , reg u64 ctr
		 , reg ptr u8[REJ_BUFLEN] buf	
		 , reg u64 pos	// pos. in buf (bytes)
		 , reg u256 bound ones mask idx8	// consts
                 , reg ptr u8[2048] idxp	// const
                 ) -> reg ptr u16[KYBER_N]
                    , reg u64 // ctr
                    , reg u64 // pos
{
  reg u256 f0 f1 g0 g1 g2 g3;
  reg u128 l h;
  reg u64 t64 t64_1 t64_2 t64_3;
  reg u64 good;

  f0 = #VPERMQ(buf.[u256 (int)pos], 0x94);
  f1 = #VPERMQ(buf.[u256 24 + (int)pos], 0x94);
  f0 = #VPSHUFB_256(f0, idx8);
  f1 = #VPSHUFB_256(f1, idx8);
  g0 = #VPSRL_16u16(f0, 4);
  g1 = #VPSRL_16u16(f1, 4);
  f0 = #VPBLEND_16u16(f0, g0, 0xAA);
  f1 = #VPBLEND_16u16(f1, g1, 0xAA);
  f0 = #VPAND_256(f0, mask);
  f1 = #VPAND_256(f1, mask);
  // f0 \bits16 k = u12buf buf (pos/3*2 + k)
  // f1 \bits16 k = u12buf buf (pos/3*2 + 16 + k)
  
  g0 = #VPCMPGT_16u16(bound, f0);
  g1 = #VPCMPGT_16u16(bound, f1);

  g0 = #VPACKSS_16u16(g0, g1);
  // g0 \bits8 k <> 0 <=> u12buf (pos/3*2 + k) < q
  good = #VPMOVMSKB_u256u64(g0);
  
  t64 = good;
  t64 &= 0xFF;
  g0 = (256u) #VMOV(idxp[u64 (int)t64]);
  // take 8 (bytes_of g0) = filter (index (<p) (?)
  
  t64_1 = good;
  t64_1 >>= 16;
  t64_1 &= 0xFF;
  l = #VMOV(idxp[u64 (int)t64_1]);
  // take 8 (bytes_of l) = filter (index (<p) (?)
  
  t64_2 = good;
  t64_2 >>= 8;
  t64_2 &= 0xFF;
  g1 = (256u) #VMOV(idxp[u64 (int)t64_2]);
  // take 8 (bytes_of g1) = filter (index (<p) (?)
  
  t64_3 = good;
  t64_3 >>= 24;
  t64_3 &= 0xFF;
  h = #VMOV(idxp[u64 (int)t64_3]);
  // take 8 (bytes_of h) = filter (index (<p) (?)
  
  g0 = #VINSERTI128(g0, l, 1);
  // bytes_of g0 = filter (index (<p) ??

  _, _, _, _, _, t64 = #POPCNT_64(t64);
  _, _, _, _, _, t64_1 = #POPCNT_64(t64_1);
  t64 += ctr; // incorpora ctr
  
  g1 = #VINSERTI128(g1, h, 1);
  // bytes_of g1 = filter (index (<p) ??
  
  t64_1 += t64;
  _, _, _, _, _, t64_2 = #POPCNT_64(t64_2);
  t64_2 += t64_1;
  _, _, _, _, _, t64_3 = #POPCNT_64(t64_3);
  t64_3 += t64_2;

  g2 = #VPADD_32u8(g0, ones);
  g0 = #VPUNPCKL_32u8(g0, g2);
  g3 = #VPADD_32u8(g1, ones);
  g1 = #VPUNPCKL_32u8(g1, g3);
  
  f0 = #VPSHUFB_256(f0, g0);
  // take (t64_1-ctr) (words_of f0) = filter (<p) ???
  f1 = #VPSHUFB_256(f1, g1);
  // take (t64_3-ctr) (words_of f1) = filter (<p) ???
  
  // rp.[u128 2*(int)ctr] = (128u)f0;
  l = (128u) f0;
  rp = __write_u128_boundchk(rp, ctr, l);

  h = #VEXTRACTI128(f0, 1);
  rp = __write_u128_boundchk(rp, t64, h);

  l = (128u) f1;
  rp = __write_u128_boundchk(rp, t64_1, l);

  h = #VEXTRACTI128(f1, 1);
  rp = __write_u128_boundchk(rp, t64_2, h);
  
  ctr = t64_3; // notice that 'ctr' might exceed 256 (but no entries are written above 'rp[255]')

  pos += 48;
  
  return rp, ctr, pos;
}

inline /* processes 1*24u8 (1*8*3u8) bytes
       */ 
fn __process1u192( reg ptr u16[KYBER_N] rp
		 , reg u64 ctr
		 , reg ptr u8[REJ_BUFLEN] buf	
		 , reg u64 pos	// pos. in buf (bytes)
		 , reg u256 bound ones mask idx8	// consts
                 , reg ptr u8[2048] idxp	// const
                 ) -> reg ptr u16[KYBER_N]
                    , reg u64 // ctr
                    , reg u64 // pos
{
  reg u256 f0 g0 g2;
  reg u128 l h;
  reg u64 t64 t64_1 t64_2 t64_3;
  reg u64 good;
  reg u64 mask55555555;

  mask55555555 = 0x55555555;


  f0 = #VPERMQ(buf.[u256 (int)pos], 0x94);
  f0 = #VPSHUFB_256(f0, idx8);
  g0 = #VPSRL_16u16(f0, 4);
  f0 = #VPBLEND_16u16(f0, g0, 0xAA);
  f0 = #VPAND_256(f0, mask);
  g0 = #VPCMPGT_16u16(bound, f0);

  good = #VPMOVMSKB_u256u64(g0);

  good = #PEXT_64(good, mask55555555);
  
  t64 = good;
  t64 &= 0xFF;
  g0 = (256u) #VMOV(idxp[u64 (int)t64]);
  // take 8 (bytes_of g0) = filter (index (<p) (?)
  
  t64_1 = good;
  t64_1 >>= 8;
  t64_1 &= 0xFF;
  l = #VMOV(idxp[u64 (int)t64_1]);
  // take 8 (bytes_of l) = filter (index (<p) (?)

  g0 = #VINSERTI128(g0, l, 1);
  // bytes_of g0 = filter (index (<p) ??

  _, _, _, _, _, t64 = #POPCNT_64(t64);
  _, _, _, _, _, t64_1 = #POPCNT_64(t64_1);
  t64 += ctr; // incorpora ctr
  
  t64_1 += t64;

  g2 = #VPADD_32u8(g0, ones);
  g0 = #VPUNPCKL_32u8(g0, g2);
  f0 = #VPSHUFB_256(f0, g0);

  l = (128u) f0;
  rp = __write_u128_boundchk(rp, ctr, l);
  h = #VEXTRACTI128(f0, 1);
  rp = __write_u128_boundchk(rp, t64, h);

  ctr = t64_1; // notice that 'ctr' might exceed 256 (but no entries are written above 'rp[255]')

  pos += 24;
  
  return rp, ctr, pos;
}

#[returnaddress="stack"]
fn _rej_uniform_avx( reg ptr u16[KYBER_N] rp  // added some slack to allow 128bit stores for the last items
                   , reg ptr u8[REJ_BUFLEN] buf // added some slack to allow 256bit read for the last 24byte chunk
                   ) -> reg u64
                      , reg ptr u16[KYBER_N]
{
  reg u256 bound ones mask idx8;
  reg ptr u8[2048] idxp;
  reg u64 pos ctr;
  reg u8 fl1 fl2;
  reg bool cf zf b;

  reg u16 val0 val1;
  reg u16 t16;


  // constants
  idxp = ru_idx;
  bound = jqx16[u256 0];
  ctr = 0;
  pos = 0;
  ones = #VPBROADCAST_32u8(ru_ones_s);
  mask = #VPBROADCAST_16u16(ru_mask_s);
  idx8 = ru_idx8_s[u256 0];

  // First stage: process 2u192 (48 bytes) from buf, potentially producing 32 entries
    _, cf, _, _, zf = #CMP_64(ctr, KYBER_N - 1); // always true!
  fl1 = #SETcc(cf || zf);
  _, _, _, _, b = #TEST_8(fl1, fl1);
  while (!b) { /* it performs at least 8 iterations (very, very unlikely! it would
                 mean that less than 32 entries were rejected). It can also 
		 perform 9 iterations, processing 432 bytes, but still with small
                 prob. (0.03%). Most likelly, it performs 10 iterations (processing
                 488 bytes), and filling the coef. array with prob. 74,76%. */
    rp, ctr, pos = __process2u192(rp, ctr, buf, pos, bound, ones, mask, idx8, idxp);

    _, cf, _, _, zf = #CMP_64(ctr, KYBER_N - 1);
    fl1 = #SETcc(cf || zf);
    _, cf, _, _, zf = #CMP_64(pos, REJ_BUFLEN - 48);
    fl2 = #SETcc(cf || zf);
     _, _, _, _, b = #TEST_8(fl1, fl2);
  }
  // 8 : pos = 384[256]  --  Pr ~ 0
  // 9 : pos = 432[288]  --  Pr = 0.03%
  // 10: pos = 480[320]  --  Pr = 74.68%

  if ( ctr < 256) { // final (half) pass
    rp, ctr, pos = __process1u192(rp, ctr, buf, pos, bound, ones, mask, idx8, idxp);
  }
  // 10.5: pos = 504[336] -- Pr = 99.17%

/*
  if (ctr > 256) {
    ctr = 256;
  }
*/

  return ctr, rp; // remark: ctr might be larger than 256
}






inline
fn __gen_entry1x( reg ptr u16[KYBER_N] rp
		, reg ptr u8[34] inbuf0
//		, reg ptr u8[REJ_BUFLEN] buf0
                ) -> reg ptr u16[KYBER_N] //, reg ptr u8[REJ_BUFLEN]
{
  stack u8[REJ_BUFLEN] buf;
  reg u256[7] st0;
  stack u256[7] st0_s;
  reg u64 ctr0;
  stack u64 ctr0_s;

  st0 = __shake128_absorb34_avx2(inbuf0);
  st0, buf = __shake128_squeeze3blocks_avx2(st0, buf);

  st0_s = st0;
  ctr0, rp = _rej_uniform_avx(rp, buf);
  st0 = st0_s;

  while( ctr0 <= KYBER_N - 1 ) {
    ctr0_s = ctr0;
    st0, buf[0:SHAKE128_RATE] = __shake128_squeezeblock_avx2(st0, buf[0:SHAKE128_RATE]);
    ctr0 = ctr0_s;
    ctr0, rp = __rej_uniform(rp, ctr0, buf[0:SHAKE128_RATE], 1);
  }

  return rp;//, buf0;
}


//#[returnaddress="stack"]
inline
fn __gen_entry4x( reg ptr u16[4*KYBER_N] rp
	       , reg ptr u8[34] inbuf0 inbuf1 inbuf2 inbuf3
//	       , reg ptr u8[REJ_BUFLEN] buf0 buf1 buf2 buf3
               ) -> reg ptr u16[4*KYBER_N] //, reg ptr u8[REJ_BUFLEN], reg ptr u8[REJ_BUFLEN], reg ptr u8[REJ_BUFLEN], reg ptr u8[REJ_BUFLEN]
{
  // keccack state
  stack u256[25] state4x;
  stack u256[7] st0_s, st1_s, st2_s, st3_s;
  reg u256[7] st;
  // 
  stack u8[REJ_BUFLEN] buf0 buf1 buf2 buf3;
  reg u64 ctr0 ctr1 ctr2 ctr3 tmp;
  stack u64 ctr0_s ctr1_s ctr2_s ctr3_s;
  reg u8 flg0 flg1 flg2;
  reg bool cf zf;

  state4x = __shake128_absorb4x_34(state4x, inbuf0, inbuf1, inbuf2, inbuf3);
  state4x, buf0, buf1, buf2, buf3 = __shake128_squeeze3blocks_4x(state4x, buf0, buf1, buf2, buf3);

  tmp, rp[0*KYBER_N:KYBER_N] = _rej_uniform_avx(rp[0*KYBER_N:KYBER_N], buf0);
  ctr0 = tmp;
  tmp, rp[1*KYBER_N:KYBER_N] = _rej_uniform_avx(rp[1*KYBER_N:KYBER_N], buf1);
  ctr1 = tmp;
  tmp, rp[2*KYBER_N:KYBER_N] = _rej_uniform_avx(rp[2*KYBER_N:KYBER_N], buf2);
  ctr2 = tmp;
  ctr3, rp[3*KYBER_N:KYBER_N] = _rej_uniform_avx(rp[3*KYBER_N:KYBER_N], buf3);

  _, cf, _, _, zf = #CMP_64(ctr0, KYBER_N - 1);
  flg0 = #SETcc(cf || zf); //SETBE
  _, cf, _, _, zf = #CMP_64(ctr1, KYBER_N - 1);
  flg1 = #SETcc(cf || zf);
  _, _, _, _, _, flg0 = #OR_8(flg0, flg1);
  _, cf, _, _, zf = #CMP_64(ctr2, KYBER_N - 1);
  flg1 = #SETcc(cf || zf);
  _, cf, _, _, zf = #CMP_64(ctr3, KYBER_N - 1);
  flg2 = #SETcc(cf || zf);
  _, _, _, _, _, flg1 = #OR_8(flg1, flg2);
  _, _, _, _, zf, _ = #OR_8(flg0, flg1);

  if (!zf) { /* test fails in 96.71% of runs... */

    st0_s, st1_s, st2_s, st3_s = __st4x_unpack_avx2(state4x);

    ctr1_s = ctr1;
    ctr2_s = ctr2;
    ctr3_s = ctr3;
    st = st0_s;
    while( ctr0 <= KYBER_N-1 ) {
      ctr0_s = ctr0;
      st, buf0[0:SHAKE128_RATE] = __shake128_squeezeblock_avx2(st, buf0[0:SHAKE128_RATE]);
      ctr0 = ctr0_s;
      ctr0, rp[0*KYBER_N:KYBER_N] = __rej_uniform(rp[0*KYBER_N:KYBER_N], ctr0, buf0[0:SHAKE128_RATE], 1);
    }
    ctr1 = ctr1_s;
    st = st1_s;
    while( ctr1 <= KYBER_N-1 ) {
      ctr1_s = ctr1;
      st, buf1[0:SHAKE128_RATE] = __shake128_squeezeblock_avx2(st, buf1[0:SHAKE128_RATE]);
      ctr1 = ctr1_s;
      ctr1, rp[1*KYBER_N:KYBER_N] = __rej_uniform(rp[1*KYBER_N:KYBER_N], ctr1, buf1[0:SHAKE128_RATE], 1);
    }
    ctr2 = ctr2_s;
    st = st2_s;
    while( ctr2 <= KYBER_N-1 ) {
      ctr2_s = ctr2;
      st, buf2[0:SHAKE128_RATE] = __shake128_squeezeblock_avx2(st, buf2[0:SHAKE128_RATE]);
      ctr2 = ctr2_s;
      ctr2, rp[2*KYBER_N:KYBER_N] = __rej_uniform(rp[2*KYBER_N:KYBER_N], ctr2, buf2[0:SHAKE128_RATE], 1);
    }
    ctr3 = ctr3_s;
    st = st3_s;
    while( ctr3 <= KYBER_N-1 ) {
      ctr3_s = ctr3;
      st, buf3[0:SHAKE128_RATE] = __shake128_squeezeblock_avx2(st, buf3[0:SHAKE128_RATE]);
      ctr3 = ctr3_s;
      ctr3, rp[3*KYBER_N:KYBER_N] = __rej_uniform(rp[3*KYBER_N:KYBER_N], ctr3, buf3[0:SHAKE128_RATE], 1);
    }
  }

  return rp; //, buf0, buf1, buf2, buf3;
} 

inline
fn __gen_matrix( //reg ptr u16[KYBER_K*KYBER_VECN] rr,
                 reg ptr u8[KYBER_SYMBYTES] seed, 
		 inline int transposed) -> stack u16[KYBER_K*KYBER_VECN]
{
  stack u8[34] inbuf0 inbuf1 inbuf2 inbuf3;
  stack u8[REJ_BUFLEN] buf0 buf1 buf2 buf3;
  stack u16[KYBER_K*KYBER_VECN] rr;
  reg u256 f;
  reg u8 flg0 flg1 flg2;
  reg bool cf zf;

  inline int i, j;

  f = seed[u256 0];
  inbuf0[u256 0] = f;
  inbuf1[u256 0] = f;
  inbuf2[u256 0] = f;
  inbuf3[u256 0] = f;

  if(transposed == 0)
  {
    inbuf0.[u16 KYBER_SYMBYTES] = 0x0000;
    inbuf1.[u16 KYBER_SYMBYTES] = 0x0001;
    inbuf2.[u16 KYBER_SYMBYTES] = 0x0002;
    inbuf3.[u16 KYBER_SYMBYTES] = 0x0100;
  }
  else
  {
    inbuf0.[u16 KYBER_SYMBYTES] = 0x0000;
    inbuf1.[u16 KYBER_SYMBYTES] = 0x0100;
    inbuf2.[u16 KYBER_SYMBYTES] = 0x0200;
    inbuf3.[u16 KYBER_SYMBYTES] = 0x0001;
  }

  rr[0*KYBER_N:4*KYBER_N] //, buf0, buf1, buf2, buf3 
  = __gen_entry4x(rr[0*KYBER_N:4*KYBER_N], inbuf0, inbuf1, inbuf2, inbuf3); //, buf0, buf1, buf2, buf3);

  if(transposed == 0)
  {
    inbuf0.[u16 KYBER_SYMBYTES] = 0x0101;
    inbuf1.[u16 KYBER_SYMBYTES] = 0x0102;
    inbuf2.[u16 KYBER_SYMBYTES] = 0x0200;
    inbuf3.[u16 KYBER_SYMBYTES] = 0x0201;
  }
  else
  {
    inbuf0.[u16 KYBER_SYMBYTES] = 0x0101;
    inbuf1.[u16 KYBER_SYMBYTES] = 0x0201;
    inbuf1.[u16 KYBER_SYMBYTES] = 0x0002;
    inbuf2.[u16 KYBER_SYMBYTES] = 0x0102;
  }

  rr[4*KYBER_N:4*KYBER_N] //, buf0, buf1, buf2, buf3 
  = __gen_entry4x(rr[4*KYBER_N:4*KYBER_N], inbuf0, inbuf1, inbuf2, inbuf3); //, buf0, buf1, buf2, buf3);

  inbuf0.[u16 KYBER_SYMBYTES] = 0x0202;

  rr[8*KYBER_N:KYBER_N]//, buf0
  = __gen_entry1x(rr[8*KYBER_N:KYBER_N], inbuf0); //, buf0);

  for i = 0 to KYBER_K {
    for j = 0 to KYBER_K {
      rr[i*KYBER_VECN+j*KYBER_N : KYBER_N] = _nttunpack(rr[i*KYBER_VECN+j*KYBER_N : KYBER_N]);
    }
  }

  return rr;
}



/*


inline fn __r2s(reg u256 f) -> stack u256 {
  stack u256 fs;
  fs = f;
  return f;
}


inline fn __s2r(stack u256 fs) -> reg u256 {
  reg u256 f;
  f = fs;
  return f;
}










inline
fn __gen_matrix(stack u8[KYBER_SYMBYTES] seed, inline int transposed) -> stack u16[KYBER_K*KYBER_VECN]
{
  stack u8[REJ_BUFLEN] buf0;
  stack u8[REJ_BUFLEN] buf1;
  stack u8[REJ_BUFLEN] buf2;
  stack u8[REJ_BUFLEN] buf3;
  stack u16[KYBER_K*KYBER_VECN] rr;
  stack u256[25] state;
  
  stack u256 fs;
  reg u256 f;
  reg u64 ctr0 ctr1 ctr2 ctr3 tmp;
  stack u64 ctr0_s;
  reg u8 flg0 flg1 bflg;
  reg bool cf zf;

  inline int i, j;

  f = seed[u256 0];
  buf0[u256 0] = f;
  buf1[u256 0] = f;
  buf2[u256 0] = f;
  buf3[u256 0] = f;
  fs = __r2s(f);

  if(transposed == 1)
  {
    buf0[KYBER_SYMBYTES]   = 0;
    buf0[KYBER_SYMBYTES+1] = 0;
    buf1[KYBER_SYMBYTES]   = 0;
    buf1[KYBER_SYMBYTES+1] = 1;
    buf2[KYBER_SYMBYTES]   = 0;
    buf2[KYBER_SYMBYTES+1] = 2;
    buf3[KYBER_SYMBYTES]   = 1;
    buf3[KYBER_SYMBYTES+1] = 0;
  }
  else
  {
    buf0[KYBER_SYMBYTES]   = 0;
    buf0[KYBER_SYMBYTES+1] = 0;
    buf1[KYBER_SYMBYTES]   = 1;
    buf1[KYBER_SYMBYTES+1] = 0;
    buf2[KYBER_SYMBYTES]   = 2;
    buf2[KYBER_SYMBYTES+1] = 0;
    buf3[KYBER_SYMBYTES]   = 0;
    buf3[KYBER_SYMBYTES+1] = 1;
  }

  state = __shake128_absorb4x_34(state, buf0[0:34], buf1[0:34], buf2[0:34], buf3[0:34]);
  state, buf0, buf1, buf2, buf3 = __shake128_squeeze3blocks4x(state, buf0, buf1, buf2, buf3);

  tmp, rr[0*KYBER_VECN+0*KYBER_N:KYBER_N] = _rej_uniform_avx(rr[0*KYBER_VECN+0*KYBER_N:KYBER_N], buf0);
  ctr0 = tmp;
  tmp, rr[0*KYBER_VECN+1*KYBER_N:KYBER_N] = _rej_uniform_avx(rr[0*KYBER_VECN+1*KYBER_N:KYBER_N], buf1);
  ctr1 = tmp;
  tmp, rr[0*KYBER_VECN+2*KYBER_N:KYBER_N] = _rej_uniform_avx(rr[0*KYBER_VECN+2*KYBER_N:KYBER_N], buf2);
  ctr2 = tmp;
  ctr3, rr[1*KYBER_VECN+0*KYBER_N:KYBER_N] = _rej_uniform_avx(rr[1*KYBER_VECN+0*KYBER_N:KYBER_N], buf3);

  _, cf, _, _, zf = #CMP_64(ctr0, KYBER_N - 1);
  flg0 = #SETcc(cf || zf); //SETBE

  _, cf, _, _, zf = #CMP_64(ctr1, KYBER_N - 1);
  flg1 = #SETcc(cf || zf);

  _, _, _, _, _, bflg = #OR_8(flg0, flg1);

  _, cf, _, _, zf = #CMP_64(ctr2, KYBER_N - 1);
  flg0 = #SETcc(cf || zf);

  _, cf, _, _, zf = #CMP_64(ctr3, KYBER_N - 1);
  flg1 = #SETcc(cf || zf);

  _, _, _, _, _, flg0 = #OR_8(flg0, flg1);
  _, _, _, _, _, bflg = #OR_8(flg0, bflg);

  while(bflg != 0) {
    state, buf0[0:SHAKE128_RATE], buf1[0:SHAKE128_RATE], buf2[0:SHAKE128_RATE], buf3[0:SHAKE128_RATE] = __shake128_squeezeblock4x(state, buf0[0:SHAKE128_RATE], buf1[0:SHAKE128_RATE], buf2[0:SHAKE128_RATE], buf3[0:SHAKE128_RATE]);

    ctr0, rr[0*KYBER_VECN+0*KYBER_N:KYBER_N] = __rej_uniform(rr[0*KYBER_VECN+0*KYBER_N:KYBER_N], ctr0, buf0[0:SHAKE128_RATE], SHAKE128_RATE);
    ctr1, rr[0*KYBER_VECN+1*KYBER_N:KYBER_N] = __rej_uniform(rr[0*KYBER_VECN+1*KYBER_N:KYBER_N], ctr1, buf1[0:SHAKE128_RATE], SHAKE128_RATE);
    ctr2, rr[0*KYBER_VECN+2*KYBER_N:KYBER_N] = __rej_uniform(rr[0*KYBER_VECN+2*KYBER_N:KYBER_N], ctr2, buf2[0:SHAKE128_RATE], SHAKE128_RATE);
    ctr3, rr[1*KYBER_VECN+0*KYBER_N:KYBER_N] = __rej_uniform(rr[1*KYBER_VECN+0*KYBER_N:KYBER_N], ctr3, buf3[0:SHAKE128_RATE], SHAKE128_RATE);

    _, cf, _, _, zf = #CMP_64(ctr0, KYBER_N - 1);
    flg0 = #SETcc(cf || zf);

    _, cf, _, _, zf = #CMP_64(ctr1, KYBER_N - 1);
    flg1 = #SETcc(cf || zf);

    _, _, _, _, _, bflg = #OR_8(flg0, flg1);

    _, cf, _, _, zf = #CMP_64(ctr2, KYBER_N - 1);
    flg0 = #SETcc(cf || zf);

    _, cf, _, _, zf = #CMP_64(ctr3, KYBER_N - 1);
    flg1 = #SETcc(cf || zf);

    _, _, _, _, _, flg0 = #OR_8(flg0, flg1);
    _, _, _, _, _, bflg = #OR_8(flg0, bflg);
  }
  
  f = __s2r(fs);
  buf0[u256 0] = f;
  buf1[u256 0] = f;
  buf2[u256 0] = f;
  buf3[u256 0] = f;
  fs = __r2s(f);

  if(transposed == 1)
  {
    buf0[KYBER_SYMBYTES]   = 1;
    buf0[KYBER_SYMBYTES+1] = 1;
    buf1[KYBER_SYMBYTES]   = 1;
    buf1[KYBER_SYMBYTES+1] = 2;
    buf2[KYBER_SYMBYTES]   = 2;
    buf2[KYBER_SYMBYTES+1] = 0;
    buf3[KYBER_SYMBYTES]   = 2;
    buf3[KYBER_SYMBYTES+1] = 1;
  }
  else
  {
    buf0[KYBER_SYMBYTES]   = 1;
    buf0[KYBER_SYMBYTES+1] = 1;
    buf1[KYBER_SYMBYTES]   = 2;
    buf1[KYBER_SYMBYTES+1] = 1;
    buf2[KYBER_SYMBYTES]   = 0;
    buf2[KYBER_SYMBYTES+1] = 2;
    buf3[KYBER_SYMBYTES]   = 1;
    buf3[KYBER_SYMBYTES+1] = 2;
  }

  state = __shake128_absorb4x_34(state, buf0[0:34], buf1[0:34], buf2[0:34], buf3[0:34]);
  state, buf0, buf1, buf2, buf3 = __shake128_squeeze3blocks4x(state, buf0, buf1, buf2, buf3);

  tmp, rr[1*KYBER_VECN+1*KYBER_N:KYBER_N] = _rej_uniform_avx(rr[1*KYBER_VECN+1*KYBER_N:KYBER_N], buf0);
  ctr0 = tmp;
  tmp, rr[1*KYBER_VECN+2*KYBER_N:KYBER_N] = _rej_uniform_avx(rr[1*KYBER_VECN+2*KYBER_N:KYBER_N], buf1);
  ctr1 = tmp;
  tmp, rr[2*KYBER_VECN+0*KYBER_N:KYBER_N] = _rej_uniform_avx(rr[2*KYBER_VECN+0*KYBER_N:KYBER_N], buf2);
  ctr2 = tmp;
  ctr3, rr[2*KYBER_VECN+1*KYBER_N:KYBER_N] = _rej_uniform_avx(rr[2*KYBER_VECN+1*KYBER_N:KYBER_N], buf3);

  _, cf, _, _, zf = #CMP_64(ctr0, KYBER_N - 1);
  flg0 = #SETcc(cf || zf);

  _, cf, _, _, zf = #CMP_64(ctr1, KYBER_N - 1);
  flg1 = #SETcc(cf || zf);

  _, _, _, _, _, bflg = #OR_8(flg0, flg1);

  _, cf, _, _, zf = #CMP_64(ctr2, KYBER_N - 1);
  flg0 = #SETcc(cf || zf);

  _, cf, _, _, zf = #CMP_64(ctr3, KYBER_N - 1);
  flg1 = #SETcc(cf || zf);

  _, _, _, _, _, flg0 = #OR_8(flg0, flg1);
  _, _, _, _, _, bflg = #OR_8(flg0, bflg);


  while(bflg != 0) {
    state, buf0[0:SHAKE128_RATE], buf1[0:SHAKE128_RATE], buf2[0:SHAKE128_RATE], buf3[0:SHAKE128_RATE] = __shake128_squeezeblock4x(state, buf0[0:SHAKE128_RATE], buf1[0:SHAKE128_RATE], buf2[0:SHAKE128_RATE], buf3[0:SHAKE128_RATE]);

    ctr0, rr[1*KYBER_VECN+1*KYBER_N:KYBER_N] = __rej_uniform(rr[1*KYBER_VECN+1*KYBER_N:KYBER_N], ctr0, buf0[0:SHAKE128_RATE], SHAKE128_RATE);
    ctr1, rr[1*KYBER_VECN+2*KYBER_N:KYBER_N] = __rej_uniform(rr[1*KYBER_VECN+2*KYBER_N:KYBER_N], ctr1, buf1[0:SHAKE128_RATE], SHAKE128_RATE);
    ctr2, rr[2*KYBER_VECN+0*KYBER_N:KYBER_N] = __rej_uniform(rr[2*KYBER_VECN+0*KYBER_N:KYBER_N], ctr2, buf2[0:SHAKE128_RATE], SHAKE128_RATE);
    ctr3, rr[2*KYBER_VECN+1*KYBER_N:KYBER_N] = __rej_uniform(rr[2*KYBER_VECN+1*KYBER_N:KYBER_N], ctr3, buf3[0:SHAKE128_RATE], SHAKE128_RATE);

    _, cf, _, _, zf = #CMP_64(ctr0, KYBER_N - 1);
    flg0 = #SETcc(cf || zf);

    _, cf, _, _, zf = #CMP_64(ctr1, KYBER_N - 1);
    flg1 = #SETcc(cf || zf);

    _, _, _, _, _, bflg = #OR_8(flg0, flg1);

    _, cf, _, _, zf = #CMP_64(ctr2, KYBER_N - 1);
    flg0 = #SETcc(cf || zf);

    _, cf, _, _, zf = #CMP_64(ctr3, KYBER_N - 1);
    flg1 = #SETcc(cf || zf);

    _, _, _, _, _, flg0 = #OR_8(flg0, flg1);
    _, _, _, _, _, bflg = #OR_8(flg0, bflg);
  }

  f = __s2r(fs);
  buf0[u256 0] = f;
  buf0[KYBER_SYMBYTES]   = 2;
  buf0[KYBER_SYMBYTES+1] = 2;

  state[u64 0:25] = _shake128_absorb34(state[u64 0:25], buf0[0:34]);
  state[u64 0:25], buf0 = __shake128_squeezenblocks(state[u64 0:25], buf0);

  ctr0, rr[2*KYBER_VECN+2*KYBER_N:KYBER_N] = _rej_uniform_avx(rr[2*KYBER_VECN+2*KYBER_N:KYBER_N], buf0);

  _, cf, _, _, zf = #CMP_64(ctr0, KYBER_N - 1);
  bflg = #SETcc(cf || zf);

  while(bflg != 0) {
    ctr0_s = ctr0;
    state[u64 0:25], buf0[0:SHAKE128_RATE] = _shake128_squeezeblock(state[u64 0:25], buf0[0:SHAKE128_RATE]);
    ctr0 = ctr0_s;

    ctr0, rr[2*KYBER_VECN+2*KYBER_N:KYBER_N] = __rej_uniform(rr[2*KYBER_VECN+2*KYBER_N:KYBER_N], ctr0, buf0[0:SHAKE128_RATE], SHAKE128_RATE);

    _, cf, _, _, zf = #CMP_64(ctr0, KYBER_N - 1);
    bflg = #SETcc(cf || zf);
  }

  for i = 0 to KYBER_K
  {
    for j = 0 to KYBER_K
    {
      rr[i*KYBER_VECN+j*KYBER_N:KYBER_N] = _nttunpack(rr[i*KYBER_VECN+j*KYBER_N:KYBER_N]);
    }
  }

  return rr;
}



*/
